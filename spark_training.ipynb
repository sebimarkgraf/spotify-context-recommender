{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.conf\n",
        "influxdb.url https://css21.teco.edu\n",
        "influxdb.token d5oSFVlZ-7TuaJgq4XYosp-6E5Bh_6MsJAit7GbcshHdUh7mKy5v-pFGfH4DGg775t_FwpK7pTsKDItRiM9nJQ\\u003d\\u003d\n",
        "influxdb.org css21\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%influxdb(saveAs\\u003d\"events\")\n",
        "result \\u003d from(bucket: \"extended_labels\")\n",
        "      |\\u003e range(start: -30d, stop: now())\n",
        "      |\\u003e filter(fn: (r) \\u003d\\u003e r.label !\\u003d \"testing\")\n",
        "      |\\u003e pivot(rowKey: [\"_time\", \"label\", \"subject\"], columnKey: [\"_field\", \"_measurement\"], valueColumn: \"_value\")\n",
        "     \n",
        " yield result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "from io import StringIO\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "df =  df.drop(columns\\u003d[\"_start\", \"result\", \"table\", \"_stop\"])\n",
        "df  spark.createDataFrame(df)\n",
        "z.show(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col,  window, mean, sum as _sum, max as _max, min as _min, var_samp, to_timestamp\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "\n",
        "df = df.withColumn(\"_time\", to_timestamp(\"_time\"))\n",
        "w = window(\"_time\", \"1 seconds\")\n",
        "\n",
        "aggregate = [\"alpha_devicemotion\", \"beta_devicemotion\", \"gamma_devicemotion\", \"x_devicemotion\", \"y_devicemotion\", \"z_devicemotion\", \"alpha_deviceorientation\", \"beta_deviceorientation\", \"gamma_deviceorientation\"] \n",
        "funs = [mean, _sum, _max, var_samp, _min]\n",
        "\n",
        "exprs = [f(col(c)) for f in funs for c in aggregate]\n",
        "\n",
        "grouped = df.fillna(0).groupBy([w, \"subject\", \"label\"]).agg(*exprs)\n",
        "grouped = grouped.drop(\"subject\", \"window\")\n",
        "grouped\n",
        "\n",
        "z.show(grouped)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.ipyspark\n",
        "from pyspark.sql.types import StringType, DoubleType\n",
        "\n",
        "num_cols = [f.name for f in grouped.schema.fields if isinstance(f.dataType, DoubleType)]\n",
        "\n",
        "num_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.ipyspark\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.feature import StringIndexer,  VectorIndexer, VectorAssembler\n",
        "\n",
        "#indexer \\u003d StringIndexer(inputCol\\u003d\"label\", outputCol\\u003d\"class\")\n",
        "#classifier \\u003d DecisionTreeClassifier(labelCol\\u003d\"class\")\n",
        "#pipeline \\u003d Pipeline(stages \\u003d [indexer, classifier])\n",
        "#pipeline.fit(grouped)\n",
        "\n",
        "# Index labels, adding metadata to the label column.\n",
        "# Fit on whole dataset to include all labels in index.\n",
        "labelIndexer \\u003d StringIndexer(inputCol\\u003d\"label\", outputCol\\u003d\"indexedLabel\").fit(grouped)\n",
        "# Automatically identify categorical features, and index them.\n",
        "# We specify maxCategories so features with \\u003e 4 distinct values are treated as continuous.\n",
        "featureAssembler \\u003d VectorAssembler(inputCols \\u003d num_cols, outputCol \\u003d \\u0027features\\u0027)\n",
        "\n",
        "# Split the data into training and test sets (30% held out for testing)\n",
        "(trainingData, testData) \\u003d grouped.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Train a DecisionTree model.\n",
        "dt \\u003d DecisionTreeClassifier(labelCol\\u003d\"indexedLabel\", featuresCol\\u003d\"features\")\n",
        "\n",
        "# Chain indexers and tree in a Pipeline\n",
        "pipeline \\u003d Pipeline(stages\\u003d[labelIndexer, featureAssembler, dt])\n",
        "\n",
        "# Train model.  This also runs the indexers.\n",
        "model \\u003d pipeline.fit(trainingData)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.ipyspark\n",
        "\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Make predictions.\n",
        "predictions \\u003d model.transform(testData)\n",
        "\n",
        "# Select example rows to display.\n",
        "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
        "\n",
        "# Select (prediction, true label) and compute test error\n",
        "evaluator \\u003d MulticlassClassificationEvaluator(\n",
        "    labelCol\\u003d\"indexedLabel\", predictionCol\\u003d\"prediction\", metricName\\u003d\"accuracy\")\n",
        "accuracy \\u003d evaluator.evaluate(predictions)\n",
        "print(\"Test Error \\u003d %g \" % (1.0 - accuracy))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "\n",
        "model.write().save(\"/notebook/ks_context\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.ipyspark\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "name": "Kontextsensitive Eventdetection"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}